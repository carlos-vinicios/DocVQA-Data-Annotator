{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/carlos_rocha/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/carlos_rocha/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/carlos_rocha/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de stopwords: 560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos_rocha/Documentos/Pesquisa/DocVQA-Data-Annotator/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os, random, re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.enums import DocumentSegments\n",
    "\n",
    "from controller.ocr import OcrData\n",
    "from controller.document_segmentation import DocumentSegmentation\n",
    "from controller.table_segmentation import TableSegmentation\n",
    "from controller.prompt_pipeline import PromptPipeline\n",
    "\n",
    "from learning.table_detection.microsoft_TART import TableDetection\n",
    "from learning.layout_analysis.pickle_file import SegmenterModel\n",
    "\n",
    "from model.image_data import ImageData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCR_DATA_PATH = \"samples/ocr\"\n",
    "PAGE_IMAGE_PATH = \"samples/images\"\n",
    "\n",
    "table_detection_model = \"microsoft/table-transformer-structure-recognition-v1.1-all\"\n",
    "\n",
    "seg_model = SegmenterModel(PAGE_IMAGE_PATH)\n",
    "table_detector = TableDetection(table_detection_model)\n",
    "\n",
    "def extract_document_data(page_path: str, ticker: str, year: int, page: int):\n",
    "    page_data = ImageData(page_path)\n",
    "    #como estamos trabalhando só com os demonstrativos\n",
    "    ocr_filename = f\"{ticker}_demonstrativo_{year}.json\"\n",
    "    ocr_data = OcrData(os.path.join(OCR_DATA_PATH, ocr_filename), page, True)\n",
    "    \n",
    "    doc_segmentation = DocumentSegmentation(page_data, seg_model, ocr_data)\n",
    "    \n",
    "    table_segments = []\n",
    "    prompt_data = \"\"\n",
    "    table_id = 1\n",
    "    text_id = 1\n",
    "    for segment in doc_segmentation.segments:\n",
    "        if segment.seg_type == DocumentSegments.TABLE:\n",
    "            table = TableSegmentation(page_data, segment, table_detector, table_id)\n",
    "            if len(table.get_table_text().strip()) > 0:\n",
    "                table_segments.append(table)\n",
    "                prompt_data += f\"\\nTABELA {table_id}:\\n\" + table.get_table_text() + \"\\n\"\n",
    "                table_id += 1\n",
    "        elif segment.seg_type == DocumentSegments.IMAGE:\n",
    "            continue\n",
    "        else:\n",
    "            if len(segment.texts) <= 0:\n",
    "                continue\n",
    "            \n",
    "            prompt_data += f\"T{text_id}:\"\n",
    "            text_id += 1\n",
    "            for text_block in segment.texts:\n",
    "                prompt_data += text_block.text + \"\\n\"\n",
    "            prompt_data += \"\\n\"\n",
    "    \n",
    "    print(prompt_data)\n",
    "    return page_data, doc_segmentation, table_segments, prompt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_examples(qtd_questions):\n",
    "    examples = \"\"\n",
    "    for i in range(1, qtd_questions+1):\n",
    "        examples += \"    {i} - pergunta: {pergunta} | resposta: {resposta} | região do texto: {região}\\n\"\\\n",
    "            .replace(\"{i}\", str(i))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "def annotate(llm_model_name: str, llm_model):\n",
    "    numbers_pattern = re.compile(r'[^0-9]') #identificar char não numéricos\n",
    "    page_files = glob(os.path.join(PAGE_IMAGE_PATH, \"*.jpg\"))\n",
    "    random.shuffle(page_files)\n",
    "    \n",
    "    for page_path in tqdm(page_files):        \n",
    "        page_filename = page_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        filename_metadatas = page_filename.split(\"_\") # o nome das imagens devem ser: ticker_ano_pagina.jpg\n",
    "        ticker = filename_metadatas[0]\n",
    "        year = int(numbers_pattern.sub('', filename_metadatas[1]))\n",
    "        page = int(numbers_pattern.sub('', filename_metadatas[2]))\n",
    "        \n",
    "        page_data, doc_segmentation, table_segments, prompt_data = extract_document_data(page_path, ticker, year, page)\n",
    "        print(prompt_data)\n",
    "        return\n",
    "\n",
    "        #criando o prompt para geração das perguntas e respostas\n",
    "        # generator_prompts = sorted(glob(\"prompt/qa_agent/*.txt\"))\n",
    "        # with open(generator_prompts[-1]) as prompt_file:\n",
    "        with open('/home/carlos/Documentos/Pesquisa/DocumentUnderstanding/prompt/qa_agent_ablation/ablation_3.txt') as prompt_file:\n",
    "            prompt_str = prompt_file.read()\n",
    "        \n",
    "        data = {\n",
    "            \"{dominio}\": \"financeiro\",\n",
    "            \"{prompt_data}\": prompt_data,\n",
    "            \"{qtd_questions}\": \"3\",\n",
    "            \"{question_examples}\": generate_qa_examples(3)\n",
    "        }\n",
    "        \n",
    "        prompt_pipe = PromptPipeline(prompt_str)\n",
    "        prompt_pipe.add_data_to_prompt(data)\n",
    "        text_blocks = doc_segmentation.filter_segments(DocumentSegments.TEXT)\n",
    "        \n",
    "        try:\n",
    "            # print(page_filename)\n",
    "            llm_model_responses, llm_model_usage = llm_model.call(prompt_pipe)\n",
    "            # print(llm_model_responses)\n",
    "            parsed_responses = prompt_pipe.parse_prompt_response(\n",
    "                llm_model_responses, text_blocks, \n",
    "                table_segments\n",
    "            )\n",
    "            questions = []\n",
    "            for resp in parsed_responses:\n",
    "                questions.append(resp.to_dict())\n",
    "            \n",
    "            width, height = page_data.image.size\n",
    "            annotation = {\n",
    "                \"ticker\": ticker,\n",
    "                \"filename\": page_filename,\n",
    "                \"page\": page,\n",
    "                \"page_size\": {\"width\": width, \"height\": height},\n",
    "                \"model\": llm_model_name,\n",
    "                \"questions\": questions,\n",
    "                \"cost\": llm_model_usage.to_dict() if llm_model_usage is not None else llm_model_usage,\n",
    "                \"review_counts\": 0,\n",
    "                \"review_costs\": [],\n",
    "            }\n",
    "\n",
    "            annotations_col.insert_one(annotation)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Falha no arquivo:\", page_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotations Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.language_model.base import LanguageModelInterface, GEMINI_PRICE\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm_model_name = \"gemini-1.5-flash-001\"\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=llm_model_name,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "llm_model = LanguageModelInterface(llm, GEMINI_PRICE)\n",
    "annotate(llm_model_name, llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.language_model.base import LanguageModelInterface, GPT4O_MINI_PRICE\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_model_name = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(\n",
    "    model=llm_model_name,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_model = LanguageModelInterface(llm, GPT4O_MINI_PRICE)\n",
    "annotate(llm_model_name, llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.language_model.base import LanguageModelInterface, CLAUDE_3_HAIKU_PRICE\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm_model_name = \"claude-3-haiku-20240307\"\n",
    "llm = ChatAnthropic(\n",
    "    model=llm_model_name,\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_model = LanguageModelInterface(llm, CLAUDE_3_HAIKU_PRICE)\n",
    "annotate(llm_model_name, llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.language_model.base import LanguageModelInterface, LLAMA3_PRICE\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm_model_name = \"llama3-70b-8192\"\n",
    "llm = ChatGroq(\n",
    "    model=llm_model_name,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_model = LanguageModelInterface(llm, LLAMA3_PRICE)\n",
    "annotate(llm_model_name, llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixtrall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning.language_model.base import LanguageModelInterface, MIXTRAL_MOE_8X22B_PRICE\n",
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "llm_model_name = \"accounts/fireworks/models/mixtral-8x22b-instruct\"\n",
    "llm = ChatFireworks(\n",
    "    model=llm_model_name,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_model = LanguageModelInterface(llm, MIXTRAL_MOE_8X22B_PRICE)\n",
    "annotate(llm_model_name, llm_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
